{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import library as lib\n",
    "import visualisation as viz\n",
    "\n",
    "import string\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import resample, shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import models\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sampled_balanced_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 20000\n",
    "epochs = 10  \n",
    "batch_size = 512  \n",
    "sequence_len = 200  # Maximum number of words in a sequence\n",
    "glove_dimensions = 200  # Number of dimensions of the GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Text = df.Text.apply(lib.remove_stopwords, join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [I, tried, jerky, fabulous!!!, Its, spicy, not...\n",
       "1    [Let, say, I, like, medium, roast, coffee.., N...\n",
       "2    [We, tried, purple,, green,, red, puffs,, like...\n",
       "3    [I, actually, like, tea,, I, give, three, star...\n",
       "4    [Oatmeal, raisin, cookies, favorite, type, coo...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.Text, df.Score, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=num_words,\n",
    "filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n',lower=True, split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = tk.texts_to_sequences(['wow what amazing cookies!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2921,  368,  995,  223]], dtype=int32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  1256,\n",
       "  9,\n",
       "  4356,\n",
       "  83,\n",
       "  9426,\n",
       "  967,\n",
       "  99,\n",
       "  7828,\n",
       "  3,\n",
       "  4190,\n",
       "  1650,\n",
       "  9,\n",
       "  25,\n",
       "  7,\n",
       "  1650,\n",
       "  670,\n",
       "  7,\n",
       "  138,\n",
       "  4046,\n",
       "  15,\n",
       "  1683,\n",
       "  3,\n",
       "  3443,\n",
       "  1650,\n",
       "  12189,\n",
       "  1650,\n",
       "  2,\n",
       "  824,\n",
       "  1650,\n",
       "  407,\n",
       "  17961,\n",
       "  3,\n",
       "  1650,\n",
       "  15,\n",
       "  4069,\n",
       "  2573,\n",
       "  1650,\n",
       "  347,\n",
       "  352,\n",
       "  4898],\n",
       " [28,\n",
       "  376,\n",
       "  70,\n",
       "  211,\n",
       "  927,\n",
       "  3082,\n",
       "  1325,\n",
       "  1671,\n",
       "  13090,\n",
       "  1,\n",
       "  46,\n",
       "  1070,\n",
       "  970,\n",
       "  19,\n",
       "  3378,\n",
       "  48,\n",
       "  7,\n",
       "  3051,\n",
       "  1,\n",
       "  142,\n",
       "  863,\n",
       "  13599,\n",
       "  75,\n",
       "  818,\n",
       "  1,\n",
       "  1077,\n",
       "  691,\n",
       "  81,\n",
       "  294,\n",
       "  2196,\n",
       "  43,\n",
       "  4,\n",
       "  1325,\n",
       "  7186,\n",
       "  16,\n",
       "  123,\n",
       "  38,\n",
       "  3184,\n",
       "  31,\n",
       "  33,\n",
       "  143,\n",
       "  22,\n",
       "  504],\n",
       " [1,\n",
       "  104,\n",
       "  313,\n",
       "  68,\n",
       "  1,\n",
       "  6104,\n",
       "  1,\n",
       "  77,\n",
       "  118,\n",
       "  24,\n",
       "  21,\n",
       "  87,\n",
       "  2,\n",
       "  10342,\n",
       "  51,\n",
       "  1409,\n",
       "  617,\n",
       "  18,\n",
       "  148,\n",
       "  321,\n",
       "  208,\n",
       "  245,\n",
       "  3185]]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tk, open( \"tk_20k_vocab_200_words.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63,\n",
       " 9858,\n",
       " 184,\n",
       " 155,\n",
       " 73,\n",
       " 66,\n",
       " 16,\n",
       " 78,\n",
       " 5966,\n",
       " 17,\n",
       " 2660,\n",
       " 142,\n",
       " 27,\n",
       " 291,\n",
       " 414,\n",
       " 6,\n",
       " 43,\n",
       " 125,\n",
       " 13,\n",
       " 170,\n",
       " 190,\n",
       " 912,\n",
       " 190,\n",
       " 230,\n",
       " 11,\n",
       " 8707,\n",
       " 24,\n",
       " 1,\n",
       " 5082,\n",
       " 358,\n",
       " 310,\n",
       " 222,\n",
       " 132,\n",
       " 6177]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_pad = pad_sequences(X_train_seq, maxlen=sequence_len)\n",
    "X_test_seq_pad = pad_sequences(X_test_seq, maxlen=sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_pad, y_train_oh, test_size=0.1, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,    51,   876,\n",
       "        1219,   151,   109,  1877,    52,   501,   992, 12400,  1280,\n",
       "          18,   116,    97,   437,  5277,    17,     2,   475,  4457,\n",
       "          10,    33,    59,  1335,   774,   143,  2341,     1,    96,\n",
       "        1038,  2032], dtype=int32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_emb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Proprietary Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65370 samples, validate on 7264 samples\n",
      "Epoch 1/10\n",
      "65370/65370 [==============================] - 36s 555us/step - loss: 1.4235 - categorical_accuracy: 0.3943 - val_loss: 1.2189 - val_categorical_accuracy: 0.4811\n",
      "Epoch 2/10\n",
      "65370/65370 [==============================] - 35s 534us/step - loss: 1.1326 - categorical_accuracy: 0.5322 - val_loss: 1.0879 - val_categorical_accuracy: 0.5447\n",
      "Epoch 3/10\n",
      "65370/65370 [==============================] - 35s 539us/step - loss: 0.9610 - categorical_accuracy: 0.6095 - val_loss: 1.0483 - val_categorical_accuracy: 0.5680\n",
      "Epoch 4/10\n",
      "65370/65370 [==============================] - 35s 536us/step - loss: 0.8074 - categorical_accuracy: 0.6821 - val_loss: 1.0449 - val_categorical_accuracy: 0.5854\n",
      "Epoch 5/10\n",
      "65370/65370 [==============================] - 35s 536us/step - loss: 0.6617 - categorical_accuracy: 0.7490 - val_loss: 1.0654 - val_categorical_accuracy: 0.5920\n",
      "Epoch 6/10\n",
      "65370/65370 [==============================] - 35s 530us/step - loss: 0.5396 - categorical_accuracy: 0.8024 - val_loss: 1.1198 - val_categorical_accuracy: 0.5951\n",
      "Epoch 7/10\n",
      "65370/65370 [==============================] - 35s 533us/step - loss: 0.4335 - categorical_accuracy: 0.8447 - val_loss: 1.2114 - val_categorical_accuracy: 0.5866\n",
      "Epoch 8/10\n",
      "65370/65370 [==============================] - 37s 562us/step - loss: 0.3502 - categorical_accuracy: 0.8752 - val_loss: 1.3203 - val_categorical_accuracy: 0.5873\n",
      "Epoch 9/10\n",
      "65370/65370 [==============================] - 35s 540us/step - loss: 0.2880 - categorical_accuracy: 0.8999 - val_loss: 1.4171 - val_categorical_accuracy: 0.5826\n",
      "Epoch 10/10\n",
      "65370/65370 [==============================] - 35s 542us/step - loss: 0.2420 - categorical_accuracy: 0.9166 - val_loss: 1.5512 - val_categorical_accuracy: 0.5859\n"
     ]
    }
   ],
   "source": [
    "emb_model = models.Sequential()\n",
    "emb_model.add(layers.Embedding(num_words, 200, input_length=sequence_len))\n",
    "emb_model.add(layers.Conv1D(32,\n",
    "                            5,\n",
    "                            activation='relu',\n",
    "                            input_shape=(200,1)))\n",
    "emb_model.add(layers.GlobalMaxPooling1D())\n",
    "emb_model.add(layers.Dropout(0.2))\n",
    "emb_model.add(layers.Dense(64,activation='relu',))\n",
    "emb_model.add(layers.Dropout(0.2))\n",
    "emb_model.add(layers.Dense(5, activation='softmax'))\n",
    "emb_model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[metrics.categorical_accuracy])\n",
    "emb_history = emb_model.fit(X_train_emb,\n",
    "                            y_train_emb,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(X_valid_emb, y_valid_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_model.layers[0].input_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.save('conv_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65370 samples, validate on 7264 samples\n",
      "Epoch 1/10\n",
      "65370/65370 [==============================] - 60s 925us/step - loss: 1.5144 - categorical_accuracy: 0.3446 - f1_m: 0.0392 - precision_m: 0.2434 - recall_m: 0.0221 - val_loss: 1.3382 - val_categorical_accuracy: 0.4266 - val_f1_m: 0.1837 - val_precision_m: 0.6130 - val_recall_m: 0.1083\n",
      "Epoch 2/10\n",
      "65370/65370 [==============================] - 57s 865us/step - loss: 1.2189 - categorical_accuracy: 0.4879 - f1_m: 0.3440 - precision_m: 0.6410 - recall_m: 0.2377 - val_loss: 1.1160 - val_categorical_accuracy: 0.5279 - val_f1_m: 0.4296 - val_precision_m: 0.6694 - val_recall_m: 0.3165\n",
      "Epoch 3/10\n",
      "65370/65370 [==============================] - 57s 871us/step - loss: 1.0083 - categorical_accuracy: 0.5869 - f1_m: 0.5151 - precision_m: 0.7032 - recall_m: 0.4070 - val_loss: 1.0627 - val_categorical_accuracy: 0.5698 - val_f1_m: 0.5037 - val_precision_m: 0.6743 - val_recall_m: 0.4023\n",
      "Epoch 4/10\n",
      "65370/65370 [==============================] - 57s 871us/step - loss: 0.8454 - categorical_accuracy: 0.6618 - f1_m: 0.6203 - precision_m: 0.7517 - recall_m: 0.5285 - val_loss: 1.0551 - val_categorical_accuracy: 0.5829 - val_f1_m: 0.5372 - val_precision_m: 0.6708 - val_recall_m: 0.4482\n",
      "Epoch 5/10\n",
      "65370/65370 [==============================] - 57s 866us/step - loss: 0.7096 - categorical_accuracy: 0.7238 - f1_m: 0.6979 - precision_m: 0.7908 - recall_m: 0.6249 - val_loss: 1.0856 - val_categorical_accuracy: 0.5895 - val_f1_m: 0.5609 - val_precision_m: 0.6547 - val_recall_m: 0.4908\n",
      "Epoch 6/10\n",
      "65370/65370 [==============================] - 57s 872us/step - loss: 0.5929 - categorical_accuracy: 0.7736 - f1_m: 0.7584 - precision_m: 0.8231 - recall_m: 0.7033 - val_loss: 1.1420 - val_categorical_accuracy: 0.5921 - val_f1_m: 0.5785 - val_precision_m: 0.6487 - val_recall_m: 0.5222\n",
      "Epoch 7/10\n",
      "65370/65370 [==============================] - 57s 876us/step - loss: 0.5036 - categorical_accuracy: 0.8125 - f1_m: 0.8033 - precision_m: 0.8493 - recall_m: 0.7621 - val_loss: 1.2125 - val_categorical_accuracy: 0.5907 - val_f1_m: 0.5846 - val_precision_m: 0.6406 - val_recall_m: 0.5377\n",
      "Epoch 8/10\n",
      "65370/65370 [==============================] - 61s 927us/step - loss: 0.4326 - categorical_accuracy: 0.8396 - f1_m: 0.8344 - precision_m: 0.8697 - recall_m: 0.8019 - val_loss: 1.2800 - val_categorical_accuracy: 0.5852 - val_f1_m: 0.5815 - val_precision_m: 0.6281 - val_recall_m: 0.5416\n",
      "Epoch 9/10\n",
      "65370/65370 [==============================] - 61s 936us/step - loss: 0.3796 - categorical_accuracy: 0.8610 - f1_m: 0.8568 - precision_m: 0.8851 - recall_m: 0.8304 - val_loss: 1.3792 - val_categorical_accuracy: 0.5856 - val_f1_m: 0.5813 - val_precision_m: 0.6176 - val_recall_m: 0.5492\n",
      "Epoch 10/10\n",
      "65370/65370 [==============================] - 58s 888us/step - loss: 0.3393 - categorical_accuracy: 0.8792 - f1_m: 0.8767 - precision_m: 0.8990 - recall_m: 0.8556 - val_loss: 1.4504 - val_categorical_accuracy: 0.5873 - val_f1_m: 0.5847 - val_precision_m: 0.6204 - val_recall_m: 0.5530\n"
     ]
    }
   ],
   "source": [
    "emb_adam_model = models.Sequential()\n",
    "emb_adam_model.add(layers.Embedding(num_words, 200, input_length=sequence_len))\n",
    "emb_adam_model.add(layers.Dropout(0.2))\n",
    "emb_adam_model.add(layers.Conv1D(32,\n",
    "                                 5,\n",
    "                                 activation='relu',\n",
    "                                 input_shape=(200,1)))\n",
    "emb_adam_model.add(layers.GlobalMaxPooling1D())\n",
    "emb_adam_model.add(layers.Dropout(0.2))\n",
    "emb_adam_model.add(layers.Dense(32,activation='relu',))\n",
    "emb_adam_model.add(layers.Dropout(0.2))\n",
    "emb_adam_model.add(layers.Dense(5, activation='softmax'))\n",
    "emb_adam_model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=[metrics.categorical_accuracy,f1_m,precision_m,recall_m])\n",
    "emb_adam_history = emb_adam_model.fit(X_train_emb,\n",
    "                            y_train_emb,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(X_valid_emb, y_valid_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbose_evaluate(model, X_test, y_test):\n",
    "    metrics = model.metrics_names\n",
    "    metric_scores = model.evaluate(X_test, y_test)\n",
    "    score_dict = {}\n",
    "    for i, metric in enumerate(metric_scores):\n",
    "        print(f'The {metrics[i]} score is: {metric}')\n",
    "        score_dict[metrics[i]] = metric\n",
    "    return score_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18159/18159 [==============================] - 4s 205us/step\n",
      "The loss score is: 1.42254966235816\n",
      "The categorical_accuracy score is: 0.5948565602302551\n",
      "The f1_m score is: 0.5909877419471741\n",
      "The precision_m score is: 0.6251880526542664\n",
      "The recall_m score is: 0.5612859725952148\n"
     ]
    }
   ],
   "source": [
    "verbose_evaluate(emb_adam_model, X_test_seq_pad, y_test_oh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = open(f'glove.6B/glove.6B.{glove_dimensions}d.txt')\n",
    "embedding_dict = {}\n",
    "for line in glove:\n",
    "    entries = line.split()\n",
    "    token = entries[0]\n",
    "    token_vector = np.asarray(entries[1:], dtype='float32')\n",
    "    embedding_dict[token] = token_vector\n",
    "glove.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((num_words, glove_dimensions))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    if i < sequence_len:\n",
    "        vect = embedding_dict.get(w)\n",
    "        \n",
    "        if vect is not None:\n",
    "            embedding_matrix[i] = vect\n",
    "            \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65370 samples, validate on 7264 samples\n",
      "Epoch 1/10\n",
      "65370/65370 [==============================] - 151s 2ms/step - loss: 1.3815 - categorical_accuracy: 0.4040 - f1_m: 0.1608 - precision_m: 0.4301 - recall_m: 0.1082 - val_loss: 1.1662 - val_categorical_accuracy: 0.5022 - val_f1_m: 0.3963 - val_precision_m: 0.6333 - val_recall_m: 0.2887\n",
      "Epoch 2/10\n",
      "65370/65370 [==============================] - 135s 2ms/step - loss: 1.0841 - categorical_accuracy: 0.5484 - f1_m: 0.4772 - precision_m: 0.6634 - recall_m: 0.3733 - val_loss: 1.0951 - val_categorical_accuracy: 0.5412 - val_f1_m: 0.4756 - val_precision_m: 0.6374 - val_recall_m: 0.3795\n",
      "Epoch 3/10\n",
      "65370/65370 [==============================] - 142s 2ms/step - loss: 0.9412 - categorical_accuracy: 0.6197 - f1_m: 0.5793 - precision_m: 0.7103 - recall_m: 0.4894 - val_loss: 1.0956 - val_categorical_accuracy: 0.5496 - val_f1_m: 0.5135 - val_precision_m: 0.6401 - val_recall_m: 0.4289\n",
      "Epoch 4/10\n",
      "65370/65370 [==============================] - 141s 2ms/step - loss: 0.8327 - categorical_accuracy: 0.6731 - f1_m: 0.6478 - precision_m: 0.7474 - recall_m: 0.5718 - val_loss: 1.1318 - val_categorical_accuracy: 0.5526 - val_f1_m: 0.5323 - val_precision_m: 0.6194 - val_recall_m: 0.4668\n",
      "Epoch 5/10\n",
      "65370/65370 [==============================] - 154s 2ms/step - loss: 0.7277 - categorical_accuracy: 0.7237 - f1_m: 0.7106 - precision_m: 0.7803 - recall_m: 0.6525 - val_loss: 1.1799 - val_categorical_accuracy: 0.5541 - val_f1_m: 0.5447 - val_precision_m: 0.6066 - val_recall_m: 0.4944\n",
      "Epoch 6/10\n",
      "65370/65370 [==============================] - 133s 2ms/step - loss: 0.6273 - categorical_accuracy: 0.7688 - f1_m: 0.7621 - precision_m: 0.8113 - recall_m: 0.7186 - val_loss: 1.2299 - val_categorical_accuracy: 0.5615 - val_f1_m: 0.5543 - val_precision_m: 0.6022 - val_recall_m: 0.5136\n",
      "Epoch 7/10\n",
      "65370/65370 [==============================] - 144s 2ms/step - loss: 0.5281 - categorical_accuracy: 0.8114 - f1_m: 0.8080 - precision_m: 0.8430 - recall_m: 0.7760 - val_loss: 1.3432 - val_categorical_accuracy: 0.5617 - val_f1_m: 0.5560 - val_precision_m: 0.5884 - val_recall_m: 0.5271\n",
      "Epoch 8/10\n",
      "65370/65370 [==============================] - 130s 2ms/step - loss: 0.4416 - categorical_accuracy: 0.8447 - f1_m: 0.8429 - precision_m: 0.8687 - recall_m: 0.8188 - val_loss: 1.4652 - val_categorical_accuracy: 0.5665 - val_f1_m: 0.5614 - val_precision_m: 0.5870 - val_recall_m: 0.5381\n",
      "Epoch 9/10\n",
      "65370/65370 [==============================] - 137s 2ms/step - loss: 0.3676 - categorical_accuracy: 0.8738 - f1_m: 0.8734 - precision_m: 0.8932 - recall_m: 0.8544 - val_loss: 1.5926 - val_categorical_accuracy: 0.5670 - val_f1_m: 0.5612 - val_precision_m: 0.5812 - val_recall_m: 0.5427\n",
      "Epoch 10/10\n",
      "65370/65370 [==============================] - 125s 2ms/step - loss: 0.3064 - categorical_accuracy: 0.8971 - f1_m: 0.8966 - precision_m: 0.9112 - recall_m: 0.8825 - val_loss: 1.7724 - val_categorical_accuracy: 0.5650 - val_f1_m: 0.5646 - val_precision_m: 0.5802 - val_recall_m: 0.5499\n"
     ]
    }
   ],
   "source": [
    "glove_adam_model = models.Sequential()\n",
    "glove_adam_model.add(layers.Embedding(num_words, glove_dimensions, input_length=sequence_len))\n",
    "glove_adam_model.layers[0].set_weights([embedding_matrix])\n",
    "glove_adam_model.layers[0].trainable=True\n",
    "glove_adam_model.add(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "glove_adam_model.add(layers.Dense(64, activation='relu'))\n",
    "glove_adam_model.add(layers.Dense(5, activation='softmax'))\n",
    "glove_adam_model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=[metrics.categorical_accuracy,f1_m,precision_m,recall_m])\n",
    "glove_adam_history = glove_adam_model.fit(X_train_emb,\n",
    "                            y_train_emb,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(X_valid_emb, y_valid_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65370 samples, validate on 7264 samples\n",
      "Epoch 1/10\n",
      "65370/65370 [==============================] - 133s 2ms/step - loss: 1.3639 - categorical_accuracy: 0.4108 - f1_m: 0.1828 - precision_m: 0.4365 - recall_m: 0.1220 - val_loss: 1.1454 - val_categorical_accuracy: 0.5076 - val_f1_m: 0.4177 - val_precision_m: 0.6547 - val_recall_m: 0.3069\n",
      "Epoch 2/10\n",
      "65370/65370 [==============================] - 136s 2ms/step - loss: 1.0801 - categorical_accuracy: 0.5528 - f1_m: 0.4748 - precision_m: 0.6661 - recall_m: 0.3694 - val_loss: 1.1102 - val_categorical_accuracy: 0.5337 - val_f1_m: 0.4384 - val_precision_m: 0.6705 - val_recall_m: 0.3260\n",
      "Epoch 3/10\n",
      "65370/65370 [==============================] - 129s 2ms/step - loss: 0.9506 - categorical_accuracy: 0.6189 - f1_m: 0.5679 - precision_m: 0.7133 - recall_m: 0.4722 - val_loss: 1.1146 - val_categorical_accuracy: 0.5379 - val_f1_m: 0.4792 - val_precision_m: 0.6379 - val_recall_m: 0.3839\n",
      "Epoch 4/10\n",
      "65370/65370 [==============================] - 149s 2ms/step - loss: 0.8467 - categorical_accuracy: 0.6689 - f1_m: 0.6380 - precision_m: 0.7448 - recall_m: 0.5583 - val_loss: 1.1389 - val_categorical_accuracy: 0.5509 - val_f1_m: 0.5214 - val_precision_m: 0.6225 - val_recall_m: 0.4487\n",
      "Epoch 5/10\n",
      "65370/65370 [==============================] - 129s 2ms/step - loss: 0.7524 - categorical_accuracy: 0.7154 - f1_m: 0.6972 - precision_m: 0.7775 - recall_m: 0.6322 - val_loss: 1.1918 - val_categorical_accuracy: 0.5575 - val_f1_m: 0.5348 - val_precision_m: 0.6079 - val_recall_m: 0.4775\n",
      "Epoch 6/10\n",
      "65370/65370 [==============================] - 140s 2ms/step - loss: 0.6636 - categorical_accuracy: 0.7533 - f1_m: 0.7432 - precision_m: 0.8036 - recall_m: 0.6914 - val_loss: 1.2707 - val_categorical_accuracy: 0.5586 - val_f1_m: 0.5487 - val_precision_m: 0.6001 - val_recall_m: 0.5055\n",
      "Epoch 7/10\n",
      "65370/65370 [==============================] - 136s 2ms/step - loss: 0.5992 - categorical_accuracy: 0.7805 - f1_m: 0.7738 - precision_m: 0.8242 - recall_m: 0.7293 - val_loss: 1.3621 - val_categorical_accuracy: 0.5602 - val_f1_m: 0.5460 - val_precision_m: 0.5880 - val_recall_m: 0.5096\n",
      "Epoch 8/10\n",
      "65370/65370 [==============================] - 141s 2ms/step - loss: 0.5348 - categorical_accuracy: 0.8058 - f1_m: 0.8030 - precision_m: 0.8436 - recall_m: 0.7662 - val_loss: 1.4490 - val_categorical_accuracy: 0.5570 - val_f1_m: 0.5469 - val_precision_m: 0.5826 - val_recall_m: 0.5153\n",
      "Epoch 9/10\n",
      "65370/65370 [==============================] - 130s 2ms/step - loss: 0.4773 - categorical_accuracy: 0.8279 - f1_m: 0.8260 - precision_m: 0.8603 - recall_m: 0.7945 - val_loss: 1.5795 - val_categorical_accuracy: 0.5538 - val_f1_m: 0.5511 - val_precision_m: 0.5779 - val_recall_m: 0.5268\n",
      "Epoch 10/10\n",
      "65370/65370 [==============================] - 137s 2ms/step - loss: 0.4327 - categorical_accuracy: 0.8467 - f1_m: 0.8457 - precision_m: 0.8757 - recall_m: 0.8176 - val_loss: 1.6723 - val_categorical_accuracy: 0.5581 - val_f1_m: 0.5522 - val_precision_m: 0.5774 - val_recall_m: 0.5291\n"
     ]
    }
   ],
   "source": [
    "emb_lstm_model = models.Sequential()\n",
    "emb_lstm_model.add(layers.Embedding(num_words, glove_dimensions, input_length=sequence_len))\n",
    "emb_lstm_model.add(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "emb_lstm_model.add(layers.Dense(64, activation='relu'))\n",
    "emb_lstm_model.add(layers.Dropout(0.4))\n",
    "emb_lstm_model.add(layers.Dense(5, activation='softmax'))\n",
    "emb_lstm_model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=[metrics.categorical_accuracy,f1_m,precision_m,recall_m])\n",
    "emb_lstm_history = emb_lstm_model.fit(X_train_emb,\n",
    "                            y_train_emb,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=batch_size,\n",
    "                            validation_data=(X_valid_emb, y_valid_emb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Modelling Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 51s, sys: 1.01 s, total: 1min 52s\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stemmer = PorterStemmer()\n",
    "corpus = [casual_tokenize(doc.lower()) for doc in df.Text]\n",
    "corpus_sw = [lib.remove_stopwords(doc) for doc in corpus]\n",
    "stemmed_corpus = []\n",
    "for doc in corpus_sw:\n",
    "    stemmed_corpus.append([stemmer.stem(word) for word in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_func(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                             tokenizer=dummy_func, \n",
    "                             preprocessor=dummy_func, \n",
    "                             max_features=8000)\n",
    "wv = vectorizer.fit_transform(stemmed_corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.shape[0] == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_wv = wv.toarray()\n",
    "feature_names = vectorizer.vocabulary_\n",
    "sorted_features = {k: v for k, v in sorted(feature_names.items(), key=lambda item: item[1])}\n",
    "wv_df = pd.DataFrame(dense_wv, columns = sorted_features.keys())\n",
    "wv_df['Target'] = df.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=5):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label-1] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wv_df.drop(columns='Target', axis=1)\n",
    "y = wv_df.Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "one_hot_train_labels = to_one_hot(y_train)\n",
    "one_hot_test_labels = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X_train[:6800]\n",
    "partial_x_train = X_train[6800:]\n",
    "y_val = one_hot_train_labels[:6800]\n",
    "partial_y_train = one_hot_train_labels[6800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61294, 8000) (61294, 5)\n",
      "(22699, 8000) (22699, 5)\n"
     ]
    }
   ],
   "source": [
    "print(partial_x_train.shape,partial_y_train.shape)\n",
    "print(X_test.shape,one_hot_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn = models.Sequential()\n",
    "simple_nn.add(layers.Dense(32, activation='relu', input_shape = (partial_x_train.shape[1], )))\n",
    "simple_nn.add(layers.Dropout(0.4))\n",
    "simple_nn.add(layers.Dense(32, activation='relu'))\n",
    "simple_nn.add(layers.Dropout(0.25))\n",
    "simple_nn.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics = [metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From //anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 61294 samples, validate on 6800 samples\n",
      "Epoch 1/10\n",
      "61294/61294 [==============================] - 12s 198us/step - loss: 1.4929 - categorical_accuracy: 0.3669 - val_loss: 1.3014 - val_categorical_accuracy: 0.4459\n",
      "Epoch 2/10\n",
      "61294/61294 [==============================] - 8s 128us/step - loss: 1.2099 - categorical_accuracy: 0.4962 - val_loss: 1.1253 - val_categorical_accuracy: 0.5316\n",
      "Epoch 3/10\n",
      "61294/61294 [==============================] - 8s 130us/step - loss: 1.0921 - categorical_accuracy: 0.5495 - val_loss: 1.0889 - val_categorical_accuracy: 0.5474\n",
      "Epoch 4/10\n",
      "61294/61294 [==============================] - 9s 141us/step - loss: 1.0285 - categorical_accuracy: 0.5799 - val_loss: 1.0797 - val_categorical_accuracy: 0.5537\n",
      "Epoch 5/10\n",
      "61294/61294 [==============================] - 9s 154us/step - loss: 0.9886 - categorical_accuracy: 0.6005 - val_loss: 1.0795 - val_categorical_accuracy: 0.5560\n",
      "Epoch 6/10\n",
      "61294/61294 [==============================] - 10s 164us/step - loss: 0.9489 - categorical_accuracy: 0.6208 - val_loss: 1.0852 - val_categorical_accuracy: 0.5569\n",
      "Epoch 7/10\n",
      "61294/61294 [==============================] - 8s 139us/step - loss: 0.9181 - categorical_accuracy: 0.6351 - val_loss: 1.0934 - val_categorical_accuracy: 0.5599\n",
      "Epoch 8/10\n",
      "61294/61294 [==============================] - 8s 134us/step - loss: 0.8850 - categorical_accuracy: 0.6504 - val_loss: 1.1010 - val_categorical_accuracy: 0.5597\n",
      "Epoch 9/10\n",
      "61294/61294 [==============================] - 8s 124us/step - loss: 0.8567 - categorical_accuracy: 0.6638 - val_loss: 1.1157 - val_categorical_accuracy: 0.5618\n",
      "Epoch 10/10\n",
      "61294/61294 [==============================] - 7s 121us/step - loss: 0.8297 - categorical_accuracy: 0.6774 - val_loss: 1.1239 - val_categorical_accuracy: 0.5597\n"
     ]
    }
   ],
   "source": [
    "nn_history = simple_nn.fit(partial_x_train,\n",
    "                        partial_y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
